{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "accelerator": "GPU",
        "colab": {
            "collapsed_sections": [],
            "name": "C1_W4_Lab_1_image_generator_no_validation.ipynb",
            "private_outputs": true,
            "provenance": [
                {
                    "file_id": "https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/adding_C1/C1/W4/ungraded_labs/C1_W4_Lab_1_image_generator_no_validation.ipynb",
                    "timestamp": 1639104486753
                }
            ]
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C1/W4/ungraded_labs/C1_W4_Lab_1_image_generator_no_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ],
            "metadata": {
                "azdata_cell_guid": "4cae6ab1-ae5a-4e5f-915f-aa91f385d829"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Ungraded Lab: Training with ImageDataGenerator\n",
                "In this lab, you will build and train a model on the [Horses or Humans](https://www.tensorflow.org/datasets/catalog/horses_or_humans) dataset. This contains over a thousand images of horses and humans with varying poses and filesizes. You will use the [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) class to prepare this dataset so it can be fed to a convolutional neural network.\n",
                "**IMPORTANT NOTE:** This notebook is designed to run as a Colab. Running it on your local machine might result in some of the code blocks throwing errors."
            ],
            "metadata": {
                "id": "-74XLLwqPlcw",
                "azdata_cell_guid": "4323d018-5cae-48b5-a95a-98c064446b6d"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Run the code below to download the compressed dataset `horse-or-human.zip`."
            ],
            "metadata": {
                "id": "qYFguQkJvpV3",
                "azdata_cell_guid": "4225a3bc-9d3c-434b-86a3-49baef61b3b5"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "!gdown --id 1onaG42NZft3wCE1WH0GDEbUhu75fedP5"
            ],
            "metadata": {
                "id": "RXZT2UsyIVe_",
                "azdata_cell_guid": "eb217b9a-49e9-4252-9797-0667d3f73123"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "*Troubleshooting: If you get a download error saying \"Cannot retrieve the public link of the file.\", please run the next two cells below to download the dataset. Otherwise, please skip them.*"
            ],
            "metadata": {
                "azdata_cell_guid": "051ca887-e1c7-4792-bc73-8c9e3070afe3"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "%%writefile download.sh\n",
                "\n",
                "#!/bin/bash\n",
                "fileid=\"1onaG42NZft3wCE1WH0GDEbUhu75fedP5\"\n",
                "filename=\"horse-or-human.zip\"\n",
                "html=`curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=${fileid}\"`\n",
                "curl -Lb ./cookie \"https://drive.google.com/uc?export=download&`echo ${html}|grep -Po '(confirm=[a-zA-Z0-9\\-_]+)'`&id=${fileid}\" -o ${filename}"
            ],
            "metadata": {
                "azdata_cell_guid": "8e2832f9-a7ea-499f-baa6-725a24b53e17"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# NOTE: Please only run this if downloading with gdown did not work.\n",
                "# This will run the script created above.\n",
                "!bash download.sh"
            ],
            "metadata": {
                "azdata_cell_guid": "648e9401-1dbe-4ae8-b64d-ee70b6e0281a"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "You can then unzip the archive using the [zipfile](https://docs.python.org/3/library/zipfile.html) module."
            ],
            "metadata": {
                "id": "9brUxyTpYZHy",
                "azdata_cell_guid": "61b164f1-5264-4afe-a00d-5af6a2919750"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import zipfile\n",
                "\n",
                "# Unzip the dataset\n",
                "local_zip = './horse-or-human.zip'\n",
                "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
                "zip_ref.extractall('./horse-or-human')\n",
                "zip_ref.close()"
            ],
            "metadata": {
                "id": "PLy3pthUS0D2",
                "azdata_cell_guid": "b83e73a4-2ce2-4a06-9a13-55e6232ba2cc"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "The contents of the .zip are extracted to the base directory `./horse-or-human`, which, in turn, contains `horses` and `humans` subdirectories.\n",
                "\n",
                "In short: The training set is the data that is used to tell the neural network model that 'this is what a horse looks like' and 'this is what a human looks like'.\n",
                "\n",
                "One thing to pay attention to in this sample: We do not explicitly label the images as horses or humans. You will use the ImageDataGenerator API instead -- and this is coded to automatically label images according to the directory names and structure. So, for example, you will have a 'training' directory containing a 'horses' directory and a 'humans' one. `ImageDataGenerator` will label the images appropriately for you, reducing a coding step.\n",
                "\n",
                "In other words: if a directory contains two or more subdirectories, the **ImageDataGenerator** will automatically label the files by: \n",
                "\n",
                "> 1) picking the name of each subdirectory; \n",
                "> \n",
                "> 2) accessing the images of that subdirectory;\n",
                "> \n",
                "> 3) labelling each image with the name of the subdirectory where it is saved.\n",
                "\n",
                "You can now define each of these directories:"
            ],
            "metadata": {
                "id": "o-qUPyfO7Qr8",
                "azdata_cell_guid": "f8bafcef-3e1d-48f2-b0d4-cd6ff739cbbe"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "\n",
                "# Directory with our training horse pictures\n",
                "train_horse_dir = os.path.join('./horse-or-human/horses')\n",
                "\n",
                "# Directory with our training human pictures\n",
                "train_human_dir = os.path.join('./horse-or-human/humans')"
            ],
            "metadata": {
                "id": "NR_M9nWN-K8B",
                "azdata_cell_guid": "18c8b840-8b2f-492c-93d5-c2a131ba7d71"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Now see what the filenames look like in the `horses` and `humans` training directories:"
            ],
            "metadata": {
                "id": "LuBYtA_Zd8_T",
                "azdata_cell_guid": "bc575a6e-9fab-4e8a-ab81-b0ba55231b5f"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "train_horse_names = os.listdir(train_horse_dir)\n",
                "print(train_horse_names[:10])\n",
                "\n",
                "train_human_names = os.listdir(train_human_dir)\n",
                "print(train_human_names[:10])"
            ],
            "metadata": {
                "id": "4PIP1rkmeAYS",
                "azdata_cell_guid": "f16df045-f3bf-47d4-bc2f-e9b10c3896ef"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "You can also find out the total number of horse and human images in the directories:"
            ],
            "metadata": {
                "id": "HlqN5KbafhLI",
                "azdata_cell_guid": "6c9e0511-e592-4a01-86a5-9971ba5e9152"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "print('total training horse images:', len(os.listdir(train_horse_dir)))\n",
                "print('total training human images:', len(os.listdir(train_human_dir)))"
            ],
            "metadata": {
                "id": "H4XHh2xSfgie",
                "azdata_cell_guid": "39a41ff2-42a6-44ab-8527-8903bf28679e"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Now take a look at a few pictures to get a better sense of what they look like. First, configure the `matplotlib` parameters:"
            ],
            "metadata": {
                "id": "C3WZABE9eX-8",
                "azdata_cell_guid": "06d4c27d-3409-4719-b174-1acd94e442db"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "%matplotlib inline\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.image as mpimg\n",
                "\n",
                "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
                "nrows = 4\n",
                "ncols = 4\n",
                "\n",
                "# Index for iterating over images\n",
                "pic_index = 0"
            ],
            "metadata": {
                "id": "b2_Q0-_5UAv-",
                "azdata_cell_guid": "af3986a5-c68c-4b41-b085-af91926b8af7"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Now, display a batch of 8 horse and 8 human pictures. You can rerun the cell to see a fresh batch each time:"
            ],
            "metadata": {
                "id": "xTvHzGCxXkqp",
                "azdata_cell_guid": "abff4f28-5f8a-4a7b-addb-e84ff70773d4"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
                "fig = plt.gcf()\n",
                "fig.set_size_inches(ncols * 4, nrows * 4)\n",
                "\n",
                "pic_index += 8\n",
                "next_horse_pix = [os.path.join(train_horse_dir, fname) \n",
                "                for fname in train_horse_names[pic_index-8:pic_index]]\n",
                "next_human_pix = [os.path.join(train_human_dir, fname) \n",
                "                for fname in train_human_names[pic_index-8:pic_index]]\n",
                "\n",
                "for i, img_path in enumerate(next_horse_pix+next_human_pix):\n",
                "  # Set up subplot; subplot indices start at 1\n",
                "  sp = plt.subplot(nrows, ncols, i + 1)\n",
                "  sp.axis('Off') # Don't show axes (or gridlines)\n",
                "\n",
                "  img = mpimg.imread(img_path)\n",
                "  plt.imshow(img)\n",
                "\n",
                "plt.show()\n"
            ],
            "metadata": {
                "id": "Wpr8GxjOU8in",
                "azdata_cell_guid": "88abefae-aec4-45a8-abb3-7303ddb37511"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Building a Small Model from Scratch\n",
                "\n",
                "Now you can define the model architecture that you will train.\n",
                "\n",
                "Step 1 will be to import tensorflow."
            ],
            "metadata": {
                "id": "5oqBkNBJmtUv",
                "azdata_cell_guid": "5cdf99d0-9bfe-4bf2-b0c0-92b065c65d28"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import tensorflow as tf"
            ],
            "metadata": {
                "id": "qvfZg3LQbD-5",
                "azdata_cell_guid": "4dfe9c20-021a-4b0f-b1ae-3452c665246c"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "You then add convolutional layers as in the previous example, and flatten the final result to feed into the densely connected layers. Note that because this is a two-class classification problem, i.e. a _binary classification problem_, you will end your network with a [_sigmoid_ activation](https://wikipedia.org/wiki/Sigmoid_function). This makes the output value of your network a single scalar between 0 and 1, encoding the probability that the current image is class 1 (as opposed to class 0).\n",
                "\n",
                "\\- This allows us to use a dense layer with a single neuron activated by 'sigmoid'. Notice that this is the logistic curve, so **the Dense(1, activation = 'sigmoid') is actually equivalent to the logistic regression**.\n",
                "\n",
                "\\- We could use, off course, Dense(2, activation = 'softmax'), as in previous examples.\n",
                "\n",
                "\\- For more classes, we should call a dense layer with number of neurons equals to the total of classes, activated by 'softmax'."
            ],
            "metadata": {
                "id": "BnhYCP4tdqjC",
                "azdata_cell_guid": "e59475d1-8084-4488-af95-a9fc83ed6076"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "model = tf.keras.models.Sequential([\n",
                "    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n",
                "    #image_depth = 3 for each of the R, G and B channels. In gray-scale, image_depth = 1;\n",
                "    #Then, input_shape = (300, 300, 3).\n",
                "    # This is the first convolution\n",
                "    # We will apply (3, 3) filters on the convolutions. Then, each convolution reduces the image by\n",
                "    # 3-1 = 2 pixels in the X axis; and 3-1 = 2 pixels in the Y axis.\n",
                "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
                "    tf.keras.layers.MaxPooling2D(2, 2),\n",
                "    #Max pooling divide by two each axis. If it does not result in an integer, the result will be\n",
                "    #rounded down to the closest integer lower than the result (e.g. 5.5 is rounded to 5).\n",
                "    \n",
                "    # The second convolution\n",
                "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
                "    tf.keras.layers.MaxPooling2D(2,2),\n",
                "    \n",
                "    # The third convolution\n",
                "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
                "    tf.keras.layers.MaxPooling2D(2,2),\n",
                "    \n",
                "    # The fourth convolution\n",
                "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
                "    tf.keras.layers.MaxPooling2D(2,2),\n",
                "    \n",
                "    # The fifth convolution\n",
                "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
                "    tf.keras.layers.MaxPooling2D(2,2),\n",
                "    \n",
                "    # Flatten the results to feed into a DNN\n",
                "    tf.keras.layers.Flatten(),\n",
                "    # 512 neuron hidden layer\n",
                "    tf.keras.layers.Dense(512, activation='relu'),\n",
                "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') \n",
                "    #and 1 for the other ('humans')\n",
                "    #Notice that the use of a dense layer with 1 neuron and 'sigmoid' activation is equivalent to the\n",
                "    #logistic regression. We could use a Dense with 2 neurons activated by 'softmax', instead, for\n",
                "    #accounting to the two classes.\n",
                "    #'sigmoid' is actually the logistic curve.\n",
                "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
                "    #WARNING: If we were performing classifications with more classes, we should use the 'softmax'\n",
                "    #activation function.\n",
                "])"
            ],
            "metadata": {
                "id": "PixZ2s5QbYQ3",
                "azdata_cell_guid": "931eace2-a8f6-42fb-b9ba-b276a52252ed"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Keras works with sequences and images of constant size. In this case, we defined that the size of the images are 300 x 300, but we could test other configurations for padding or limiting image size.\n",
                "\n",
                "\\- Since we are working on RGB system, each image is comprised of 3 matrices, for the red, green, and blue channels. So, the image depth = 3 bytes, instead of only 1 as in the gray-scaled images.\n",
                "\n",
                "\\- We work here with 5 convolutions due to the higher complexity of the images, but we could test the addition of more layers; or test the use of fewer convolutions."
            ],
            "metadata": {
                "azdata_cell_guid": "02402911-839f-4609-a1a4-c51e64348716"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "You can review the network architecture and the output shapes with `model.summary()`."
            ],
            "metadata": {
                "id": "s9EaFDP5srBa",
                "azdata_cell_guid": "d0d4fa41-d6a1-4f6e-a2d0-ab7a395c1efd"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "model.summary()"
            ],
            "metadata": {
                "id": "7ZKj8392nbgP",
                "azdata_cell_guid": "3364ad95-1a62-4f44-a4c6-66da9e1741f5"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "The \"output shape\" column shows how the size of your feature map evolves in each successive layer. As you saw in an earlier lesson, the convolution layers removes the outermost pixels of the image, and each pooling layer halves the dimensions.\n",
                "\n",
                "\\- After each convolution, each axis size is reduced in 2 units;\n",
                "\n",
                "> \\- That is because we applied a (3, 3) filter. When using a (N, N) filter in convolution, the output shape is reduced by (N-1) units on axis X, and (N-1) units on axis Y.\n",
                "\n",
                "\\- After each pooling, each axis size is divided by 2. If this division does not result into an integer, the dimension is rounded down to the closest integer lower than the result (e.g. 5.5 will be rounded down to 5)."
            ],
            "metadata": {
                "id": "DmtkTn06pKxF",
                "azdata_cell_guid": "61c2db04-4c50-4e9e-94a7-fb21332b6dcf"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Next, you'll configure the specifications for model training. You will train the model with the [`binary_crossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy) loss because it's a binary classification problem, and the final activation is a sigmoid. (For a refresher on loss metrics, see this [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/descending-into-ml/video-lecture).) You will use the `rmsprop` optimizer with a learning rate of `0.001`. During training, you will want to monitor classification accuracy.\n",
                "\n",
                "> \\- Notice that we could use the 'adam' optimizer. The reason why we use the 'rmsprop' is because it allows us to pass the learning rate as an hyperparameter. So, we can test the effects of using different learning rates.\n",
                "\n",
                "\\- If more classes were used, we would use the sparse\\_categorical\\_crossentropy, instead;\n",
                "\n",
                "\\- These losses are adequate for classification problems. For regressions, we should use a proper loss, such as the mean\\_squared\\_error.\n",
                "\n",
                "**NOTE**: In this case, using the [RMSprop optimization algorithm](https://wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp) is preferable to [stochastic gradient descent](https://developers.google.com/machine-learning/glossary/#SGD) (SGD), because RMSprop automates learning-rate tuning for us. (Other optimizers, such as [Adam](https://wikipedia.org/wiki/Stochastic_gradient_descent#Adam) and [Adagrad](https://developers.google.com/machine-learning/glossary/#AdaGrad), also automatically adapt the learning rate during training, and would work equally well here.)"
            ],
            "metadata": {
                "id": "PEkKSpZlvJXA",
                "azdata_cell_guid": "19c0fe24-898d-483a-aca5-161487e055df"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "from tensorflow.keras.optimizers import RMSprop\n",
                "#The only reason why we used the 'RMSProp' is because this optimizer allows us to define the\n",
                "#learning_rate as an hyperparameter, so we can visualize the effects of using different learning\n",
                "#rates. We could simply substitute it for the 'adam' optimizer, as in previous cases.\n",
                "\n",
                "model.compile(loss='binary_crossentropy',\n",
                "              optimizer=RMSprop(learning_rate=0.001),\n",
                "              metrics=['accuracy'])\n",
                "\n",
                "#'sparse_categorical_crossentropy' is a loss metric adequate for classification problems, \n",
                "#not for regression ones.\n",
                "#Here, we used the 'binary_crossentropy' because it is a binary classification, i.e., we divided\n",
                "#the images into only two different classes. For more classifications, we should return to the \n",
                "#'sparse_categorical_crossentropy'.\n",
                "#For regressions, we use 'mean_squared_error', for instance."
            ],
            "metadata": {
                "id": "8DHWhFP_uhq3",
                "azdata_cell_guid": "46140e40-b3cc-4450-b202-873b081f1d65",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Data Preprocessing\n",
                "\n",
                "Next step is to set up the data generators that will read pictures in the source folders, convert them to `float32` tensors, and feed them (with their labels) to the model. You'll have one generator for the training images and one for the validation images. These generators will yield batches of images of size 300x300 and their labels (binary).\n",
                "\n",
                "As you may already know, data that goes into neural networks should usually be normalized in some way to make it more amenable to processing by the network (i.e. it is uncommon to feed raw pixels into a ConvNet.) In this case, you will preprocess the images by normalizing the pixel values to be in the `[0, 1]` range (originally all values are in the `[0, 255]` range).\n",
                "\n",
                "In Keras, this can be done via the `keras.preprocessing.image.ImageDataGenerator` class using the `rescale` parameter. This `ImageDataGenerator` class allows you to instantiate generators of augmented image batches (and their labels) via `.flow(data, labels)` or `.flow_from_directory(directory)`."
            ],
            "metadata": {
                "id": "Sn9m9D3UimHM",
                "azdata_cell_guid": "584787f1-c16d-4440-8085-bbe6866fb5ec"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "\n",
                "# All images will be rescaled by 1./255\n",
                "train_datagen = ImageDataGenerator(rescale=1/255)\n",
                "#The ImageDataGenerator allows us to automatically normalize the images while importing then,\n",
                "#by passing the scaling factor as the method parameter (\"rescale\").\n",
                "\n",
                "# Flow training images in batches of 128 using train_datagen generator\n",
                "train_generator = train_datagen.flow_from_directory(\n",
                "        './horse-or-human/',  # This is the source directory for training images\n",
                "        target_size=(300, 300),  # All images will be resized to 300x300\n",
                "        batch_size=128,\n",
                "        # Since we use binary_crossentropy loss, we need binary labels\n",
                "        class_mode='binary')\n",
                ""
            ],
            "metadata": {
                "id": "ClebU9NJg99G",
                "azdata_cell_guid": "8377149d-6fca-43a4-a8df-faa328545dfb"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Normalization\n",
                "\n",
                "In the RGBA system, each pixel is represented by an intensity in one of the 3 channels: Red, Green, and Blue.\n",
                "\n",
                "\\- The intensities go from 0 to 255.\n",
                "\n",
                "Dividing by 255 is equivalent to applying Min-Max normalization, or normalization through division by the maximum possible value:\n",
                "\n",
                "I norm = (I - Imin)/(Imax - Imin) = (I - 0)/(255 - 0) = I/255\n",
                "\n",
                "\\- That is because the minimum possible value is zero. Then, the two methods (min-max normalization and division by the maximum possible value) get equivalent results.\n",
                "\n",
                "\\- The ImageDataGenerator allows us to automatically normalize the images while importing them, by passing the scaling factor as the method parameter (\"rescale\")."
            ],
            "metadata": {
                "azdata_cell_guid": "7ea571fa-da36-4c22-a6d5-574ee33cee86"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## flow\\_from\\_directory Method\n",
                "\n",
                "This is one of the methods of the objects created from the class ImageDataGenerator. Its arguments are:\n",
                "\n",
                "> 1. First argument: path of the directory from which the images will be imported. This directory must be divided into several subdirectories. The subdirectories represent the labels that will be assigned to each image. Notice that the images must be saved in these subdirectories, and the name of the subdirectory will be used as label for the images stored in it.\n",
                ">     \n",
                "> 2. 'target\\_size': the images will be automatically resized to fit the target\\_size, in pixels. The original files are not modified: the images are copied and resized before being processed by the Python code, so we can test different sizes. Importantly, **this parameter is analogous to pad\\_sequences (\"padding\") applied to text processing: it guarantees that the images will always have same sizes, a requirement from Tensorflow**.\n",
                ">     \n",
                "> 3. 'batch\\_size': amount of data that is processed at each epoch (training cycle). If batch size = length of dataset, all data is simultaneously processed. To break the dataset into several batches is more efficient, specially when the data is redundant.\n",
                ">     \n",
                "> 4. 'class\\_mode': how many classes are possible. Here, we set class\\_mode = 'binary' because only two classifications are allowed. Other possibilities are \"categorical\" or \"sparse\", and should be selected according to the labels provided.\n",
                ">"
            ],
            "metadata": {
                "azdata_cell_guid": "d711e4d3-4e4b-489a-b59a-70c96c041cdf"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Training\n",
                "\n",
                "You can start training for 15 epochs -- this may take a few minutes to run.\n",
                "\n",
                "Do note the values per epoch.\n",
                "\n",
                "The `loss` and `accuracy` are great indicators of progress in training. `loss` measures the current model prediction against the known labels, calculating the result. `accuracy`, on the other hand, is the portion of correct guesses.\n",
                "\n",
                "\\- Notice that the parameters of the fit method are slightly different. That is because we are now training the neural network with a different object, which was obtained from the class ImageDataGenerator."
            ],
            "metadata": {
                "id": "mu3Jdwkjwax4",
                "azdata_cell_guid": "afb23fba-b2b5-4ccb-9db0-8a10fb8518b1"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "history = model.fit(\n",
                "      train_generator,\n",
                "      steps_per_epoch=8,  \n",
                "      epochs=15,\n",
                "      verbose=1)\n",
                "# The fit parameters are different because now we are training the neural networks with a different \n",
                "# object, i.e., an object from the class ImageDataGenerator."
            ],
            "metadata": {
                "id": "Fb1_lgobv81m",
                "azdata_cell_guid": "1172245b-8de2-41f2-bdf0-7a4a7c9e6e38"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Model Prediction\n",
                "\n",
                "Now take a look at actually running a prediction using the model. This code will allow you to choose 1 or more files from your file system, upload them, and run them through the model, giving an indication of whether the object is a horse or a human.\n",
                "\n",
                "**Important Note:** Due to some compatibility issues, the following code block will result in an error after you select the images(s) to upload if you are running this notebook as a `Colab` on the `Safari` browser. For all other browsers, continue with the next code block and ignore the next one after it.\n",
                "\n",
                "_For Safari users: please comment out or skip the code block below, uncomment the next code block and run it._"
            ],
            "metadata": {
                "id": "o6vSHzPR2ghH",
                "azdata_cell_guid": "2c428540-1607-4cbe-903e-e8ed8eb3abd1"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "## CODE BLOCK FOR NON-SAFARI BROWSERS\n",
                "## SAFARI USERS: PLEASE SKIP THIS BLOCK AND RUN THE NEXT ONE INSTEAD\n",
                "\n",
                "import numpy as np\n",
                "from google.colab import files\n",
                "from keras.preprocessing import image\n",
                "\n",
                "uploaded = files.upload()\n",
                "\n",
                "for fn in uploaded.keys():\n",
                " \n",
                "  # predicting images\n",
                "  path = '/content/' + fn\n",
                "  img = image.load_img(path, target_size=(300, 300))\n",
                "  x = image.img_to_array(img)\n",
                "  x /= 255\n",
                "  x = np.expand_dims(x, axis=0)\n",
                "\n",
                "  images = np.vstack([x])\n",
                "  classes = model.predict(images, batch_size=10)\n",
                "  print(classes[0])\n",
                "    \n",
                "  if classes[0]>0.5:\n",
                "    print(fn + \" is a human\")\n",
                "  else:\n",
                "    print(fn + \" is a horse\")\n",
                " "
            ],
            "metadata": {
                "id": "DoWp43WxJDNT",
                "azdata_cell_guid": "a162bc5d-b6f3-44b1-85ae-f6a3da334f34"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "`Safari` users will need to upload the images(s) manually in their workspace. Please follow the instructions, uncomment the code block below and run it.\n",
                "\n",
                "Instructions on how to upload image(s) manually in a Colab:\n",
                "\n",
                "1. Select the `folder` icon on the left `menu bar`.\n",
                "2. Click on the `folder with an arrow pointing upwards` named `..`\n",
                "3. Click on the `folder` named `tmp`.\n",
                "4. Inside of the `tmp` folder, `create a new folder` called `images`. You'll see the `New folder` option by clicking the `3 vertical dots` menu button next to the `tmp` folder.\n",
                "5. Inside of the new `images` folder, upload an image(s) of your choice, preferably of either a horse or a human. Drag and drop the images(s) on top of the `images` folder.\n",
                "6. Uncomment and run the code block below. "
            ],
            "metadata": {
                "id": "WkLydXVZr30K",
                "azdata_cell_guid": "bc292000-8bba-4514-a92e-c8f7110531f3"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# # CODE BLOCK FOR SAFARI USERS\n",
                "\n",
                "# import numpy as np\n",
                "# from keras.preprocessing import image\n",
                "# import os\n",
                "\n",
                "# images = os.listdir(\"/tmp/images\")\n",
                "\n",
                "# print(images)\n",
                "\n",
                "# for i in images:\n",
                "#  print()\n",
                "#  # predicting images\n",
                "#  path = '/tmp/images/' + i\n",
                "#  img = image.load_img(path, target_size=(300, 300))\n",
                "#  x = image.img_to_array(img)\n",
                "#  x /= 255\n",
                "#  x = np.expand_dims(x, axis=0)\n",
                "\n",
                "#  images = np.vstack([x])\n",
                "#  classes = model.predict(images, batch_size=10)\n",
                "#  print(classes[0])\n",
                "#  if classes[0]>0.5:\n",
                "#    print(i + \" is a human\")\n",
                "#  else:\n",
                "#    print(i + \" is a horse\")"
            ],
            "metadata": {
                "id": "1_vVstx-r4jy",
                "azdata_cell_guid": "24da9eca-618d-4a0c-bb51-7db6b2969646"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Visualizing Intermediate Representations\n",
                "\n",
                "To get a feel for what kind of features your CNN has learned, one fun thing to do is to visualize how an input gets transformed as it goes through the model.\n",
                "\n",
                "You can pick a random image from the training set, and then generate a figure where each row is the output of a layer, and each image in the row is a specific filter in that output feature map. Rerun this cell to generate intermediate representations for a variety of training images."
            ],
            "metadata": {
                "id": "-8EHQyWGDvWz",
                "azdata_cell_guid": "f10513ae-e26b-4e20-b583-3bdcccf7290d"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import numpy as np\n",
                "import random\n",
                "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
                "\n",
                "# Define a new Model that will take an image as input, and will output\n",
                "# intermediate representations for all layers in the previous model after\n",
                "# the first.\n",
                "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
                "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
                "\n",
                "# Prepare a random input image from the training set.\n",
                "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
                "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
                "img_path = random.choice(horse_img_files + human_img_files)\n",
                "\n",
                "img = load_img(img_path, target_size=(300, 300))  # this is a PIL image\n",
                "x = img_to_array(img)  # Numpy array with shape (300, 300, 3)\n",
                "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 300, 300, 3)\n",
                "\n",
                "# Scale by 1/255\n",
                "x /= 255\n",
                "\n",
                "# Run the image through the network, thus obtaining all\n",
                "# intermediate representations for this image.\n",
                "successive_feature_maps = visualization_model.predict(x)\n",
                "\n",
                "# These are the names of the layers, so you can have them as part of the plot\n",
                "layer_names = [layer.name for layer in model.layers[1:]]\n",
                "\n",
                "# Display the representations\n",
                "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
                "  if len(feature_map.shape) == 4:\n",
                "\n",
                "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
                "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
                "\n",
                "    # The feature map has shape (1, size, size, n_features)\n",
                "    size = feature_map.shape[1]\n",
                "    \n",
                "    # Tile the images in this matrix\n",
                "    display_grid = np.zeros((size, size * n_features))\n",
                "    for i in range(n_features):\n",
                "      x = feature_map[0, :, :, i]\n",
                "      x -= x.mean()\n",
                "      x /= x.std()\n",
                "      x *= 64\n",
                "      x += 128\n",
                "      x = np.clip(x, 0, 255).astype('uint8')\n",
                "    \n",
                "      # Tile each filter into this big horizontal grid\n",
                "      display_grid[:, i * size : (i + 1) * size] = x\n",
                "    \n",
                "    # Display the grid\n",
                "    scale = 20. / n_features\n",
                "    plt.figure(figsize=(scale * n_features, scale))\n",
                "    plt.title(layer_name)\n",
                "    plt.grid(False)\n",
                "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
            ],
            "metadata": {
                "id": "-5tES8rXFjux",
                "azdata_cell_guid": "f20725a5-43e6-446d-9994-d76b17a8dbc3"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "You can see above how the pixels highlighted turn to increasingly abstract and compact representations, especially at the bottom grid. \n",
                "\n",
                "The representations downstream start highlighting what the network pays attention to, and they show fewer and fewer features being \"activated\"; most are set to zero. This is called _representation sparsity_ and is a key feature of deep learning. These representations carry increasingly less information about the original pixels of the image, but increasingly refined information about the class of the image. You can think of a convnet (or a deep network in general) as an information distillation pipeline wherein each layer filters out the most useful features."
            ],
            "metadata": {
                "id": "tuqK2arJL0wo",
                "azdata_cell_guid": "49863d4c-9596-46a1-8107-6f928f160a7b"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Clean Up\n",
                "\n",
                "You will continue with a similar exercise in the next lab but before that, run the following cell to terminate the kernel and free memory resources:"
            ],
            "metadata": {
                "id": "j4IBgYCYooGD",
                "azdata_cell_guid": "6183aa24-1f78-4969-9516-6eb98638c337"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import os, signal\n",
                "os.kill(os.getpid(), signal.SIGKILL)"
            ],
            "metadata": {
                "id": "651IgjLyo-Jx",
                "azdata_cell_guid": "5314716d-acc8-4666-a61f-bb2767da6248"
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}